y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
exp(logx)
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
# Plot transformed data x, logy
plot(logx,y)
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 10
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 0.6
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# I choose the following values:
coef_alpha <- 5
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 17
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
# Plot x,y
plot(x,y)
coef_beta <- 45
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 2
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 5
coef_beta <- 2
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
fit <- lm(y~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- predicted
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*exp(logx
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*exp(logx)
lines(x, fit_model, col = "blue")
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*logx
lines(x, fit_model, col = "blue")
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, fit_back, col="red")
lines(x, predicted, col="red")
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
residuals()
residuals(predicted)
residuals(fit)
qqnorm(residuals(fit))
# Model 2: log(yi) = α + βxi + εi
x <- seq(1.5, 2.5, length = 100) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*x  +  rnorm(100,  sd  =  epsiloni)
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(x,logy)
fit <- lm(logy~x)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*x)
lines(x, fit_model)
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 5
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
fit <- lm(y~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- predicted
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (same result)
fit_model <- coef_alpha + coef_beta*logx
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
# Bad fit
### Conidia discharge
cold <- c(1575, 2019, 1921, 2019, 2323)
medium <- c(2003, NULL, 1510, 1991, 1720)
warm <- c(1742, 1764, NULL, 1470, 1769)
data.frame(cold, medium, warm, colnames("Cold","Medium","Warm"))
data.frame(cold, medium, warm)
?kruskal.test
x <- c(cold, medium, warm)
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
kruskal.test(x,g)
g
boxplot(x)
boxplot(cold,medium,warm)
boxplot(cold,medium,warm, xlabel = c("Cold",
"Medium",
"Warm"))
boxplot(cold,medium,warm, xlabel = c("Cold",
"Medium",
"Warm"))
boxplot(cold,medium,warm, xlabel = c("Cold", "Medium", "Warm"))
?boxplot
boxplot(cold,medium,warm, xlab = c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, names = c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, names = c("Cold", "Medium", "Warm"), ylab = "Number of conidia")
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
kruskal.test(x,g)
?anova
anova(x)
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
# Data
cold <- c(1575, 2019, 1921, 2019, 2323)
medium <- c(2003, NULL, 1510, 1991, 1720)
warm <- c(1742, 1764, NULL, 1470, 1769)
groups <- c("cold", "medium", "warm")
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
groups <- c("Cold", "Medium", "Warm")
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
g <- factor(rep(1:3, c(5, 4, 4)),
labels = groups)
#
kruskal.test(x,g)
# Combine the data into a single data frame
data <- data.frame(value = c(cold, medium, warm), group = rep(groups, c(length(cold), length(medium), length(warm))))
# Perform the ANOVA test
anova_results <- anova(lm(value ~ group, data = data))
# Print the summary of the ANOVA test
print(anova_results)
anova(x)
# Print the summary of the ANOVA test
print(anova_results)
p_hat1 <- 0.183
p_hat2 <- 0.225
?prop.test
prop.test(c(xA,xB), c(n,n))
prop.test(0.183,0.225)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n))
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n))
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
?pwr
??pwr
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
# Power analysis of two proportions with equal n
alpha <- 0.05
power.result <- power.prop.test(n = n, p1 = xA/n, p2 = xB/n, sig.level = alpha, power = NULL)
power.result
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
# Power analysis of two proportions with equal n
alpha <- 0.05
power.result <- power.prop.test(n = n, p1 = xA/n, p2 = xB/n, sig.level = alpha, power = NULL)
p_pooled <- (xA+xB)/(2*n)
pA <- xA/n
pB <- xB/n
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
test_stat1 <- -1.96 - (z)
test_stat2 <- 1.96 - (z)
pnorm
?pnorm
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
power.result
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
test_stat1 <- -1.96 + (z)
test_stat2 <- 1.96 + (z)
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
z
test_stat1 <- -1.96 + (z)
test_stat2 <- 1.96 + (z)
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
# Power analysis of two proportions with equal n
alpha <- 0.05
power.result <- power.prop.test(n = n, p1 = xA/n, p2 = xB/n, sig.level = alpha, power = NULL)
p_pooled <- (xA+xB)/(2*n)
pA <- xA/n
pB <- xB/n
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
test_stat1 <- -1.96 + (z)
test_stat2 <- 1.96 + (z)
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
test_stat1 <- -1.96 - (z)
test_stat2 <- 1.96 - (z)
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
test_stat1
test_stat2
test_stat1 <- -1.96 + (z)
test_stat2 <- 1.96 + (z)
test_stat1
test_stat2
z <- (pB - pA)/sqrt(p_pooled * (1- p_pooled)* (1/n + 1/n))
test_stat1 <- -1.96 - (z)
test_stat2 <- 1.96 - (z)
power1 <- pnorm(test_stat1, mean = 0, sd = 1) + (1-pnorm(test_stat2, mean = 0, sd = 1))
power1
summary(lm(lameLeg ~ horse, data = D))
D <- read.delim("horse_data23.txt")
par(mfrow = c(1, 1))
boxplot(D$A ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$S ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$W ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
par(mfrow = c(3, 1))
boxplot(D$A ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$S ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$W ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
print(D[D$horse == c("B5", "B9"), ])
print(var.test(A ~ horse, data = D[D$horse == c("B5", "B9"), ]))
print(var.test(S ~ horse, data = D[D$horse == c("B4", "B6"), ]))
print(var.test(W ~ horse, data = D[D$horse == c("B1", "B7"), ]))
print(var.test(W ~ lameLeg, data = D[D$lameLeg == c("none", "right:fore"), ]))
print(kruskal.test(A ~ horse, data = D))
print(kruskal.test(S ~ horse, data = D))
summary(lm(lameLeg ~ horse, data = D))
# If horse has a significant effect on the symmetry scores
L_A <- lm(A ~ horse * lameLeg, data = D)
D <- read.delim("horse_data23.txt")
# Set working directory
setwd("/Users/clarasofiechristiansen/Documents/Github_Desktop/Horseproject")
D <- read.delim("horse_data23.txt")
par(mfrow = c(1, 1))
boxplot(D$A ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$S ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$W ~ D$lameLeg, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
par(mfrow = c(3, 1))
boxplot(D$A ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$S ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
boxplot(D$W ~ D$horse, col = c("red", "blue", "green", "yellow", "white"), horizontal = TRUE)
print(D[D$horse == c("B5", "B9"), ])
print(var.test(A ~ horse, data = D[D$horse == c("B5", "B9"), ]))
print(var.test(S ~ horse, data = D[D$horse == c("B4", "B6"), ]))
print(var.test(W ~ horse, data = D[D$horse == c("B1", "B7"), ]))
print(var.test(W ~ lameLeg, data = D[D$lameLeg == c("none", "right:fore"), ]))
print(kruskal.test(A ~ horse, data = D))
print(kruskal.test(S ~ horse, data = D))
summary(lm(lameLeg ~ horse, data = D))
summary(lm(lameLeg ~ horse, data = D))
table(lameLeg, horse)
table(lameLeg, horse, data =D)
table(lameLeg, horse, data = D)
table(D$lameLeg, D$horse)
