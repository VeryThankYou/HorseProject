plot(x,logy)
fit <- lm(logy~x)
lines(x, fit)
# Plot transformed data logy, x
plot(x,logy)
fit <- lm(logy~x)
lines(fit)
lines(predict(fit,x))
lines(predict(fit,newdata = data.frame(x=x)))
# Plot transformed data logy, x
plot(x,logy)
fit <- lm(logy~x)
lines(predict(fit,newdata = data.frame(x=x)))
source("~/.active-rstudio-document", echo=TRUE)
predict(fit,newdata = data.frame(x=x)
)
# Model 2: log(yi) = α + βxi + εi
x <- seq(1.5, 2.5, length = 100) ## make x’s
lines(predict(fit,newdata = data.frame(x=x)))
fit <- lm(logy~x)
lines(predict(fit,newdata = data.frame(x=x)))
fit
x
data.frame(x=x)
lines(predict(fit,newdata = data.frame(x=x)), col="red")
# Plot the transformed data
plot(x, logy)
# Fit a linear regression model to the data
fit <- lm(logy ~ x)
# Use the predict() function to generate predicted values for the fit
predicted <- predict(fit, newdata = data.frame(x = x))
# Add the fit to the plot using the lines() function
lines(x, predicted, col = "red")
fit <- lm(logy~x)
lines(x, predict(fit, newdata = data.frame(x=x)), col="red")
source("~/.active-rstudio-document", echo=TRUE)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back)
lines(x, fit_back, col="red")
fit_model <- exp(coef_alpha + coef_beta*x)
lines(x, fit_model)
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*x  +  rnorm(100,  sd  =  epsiloni)
y <- exp(logy) ## transform back
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(xlog,logy)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*x)
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model)
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model)
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*x)
lines(x, fit_model)
lines(x, fit_model, col = "blue")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
exp(logx)
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
# Plot transformed data x, logy
plot(logx,y)
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 10
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 0.6
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# I choose the following values:
coef_alpha <- 5
coef_beta <- 5
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 17
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
# Plot x,y
plot(x,y)
coef_beta <- 45
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 2
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 5
coef_beta <- 2
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
fit <- lm(y~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
lines(x, fit_model, col = "blue")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- predicted
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*exp(logx))
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*exp(logx
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*exp(logx)
lines(x, fit_model, col = "blue")
# They suggest: (almost same result)
fit_model <- coef_alpha + coef_beta*logx
lines(x, fit_model, col = "blue")
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, fit_back, col="red")
lines(x, predicted, col="red")
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
residuals()
residuals(predicted)
residuals(fit)
qqnorm(residuals(fit))
# Model 2: log(yi) = α + βxi + εi
x <- seq(1.5, 2.5, length = 100) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*x  +  rnorm(100,  sd  =  epsiloni)
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(x,logy)
fit <- lm(logy~x)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*x)
lines(x, fit_model)
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 2.5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 3: log(yi) = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 0.5
coef_beta <- 3
epsiloni <- 0.15
logy <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
y <- exp(logy) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,logy)
fit <- lm(logy~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- exp(predicted)
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (almost same result)
fit_model <- exp(coef_alpha + coef_beta*logx)
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
#####################################################################
# Model 4: yi = α + β * log(xi) + εi
logx <- log(seq(1.5, 5, length = 100)) ## make x’s
# I choose the following values:
coef_alpha <- 5
coef_beta <- 3
epsiloni <- 0.15
y <-  coef_alpha + coef_beta*logx  +  rnorm(100,  sd  =  epsiloni)
x <- exp(logx) ## transform back
# Plot x,y
plot(x,y)
# Plot transformed data x, logy
plot(logx,y)
fit <- lm(y~logx)
# With linear regression
predicted <- predict(fit, newdata = data.frame(x=x))
lines(logx, predicted, col="red")
# Backtransform the linear regression so it fits the x,y-plot
fit_back <- predicted
plot(x,y)
lines(x, fit_back, col="red")
# They suggest: (same result)
fit_model <- coef_alpha + coef_beta*logx
lines(x, fit_model, col = "blue")
## Q-Q plot with "wrong" linearity to check residuals
plot(x,y)
fit <- lm(y~x)
predicted <- predict(fit, newdata = data.frame(x=x))
lines(x, predicted, col="red")
qqnorm(residuals(fit))
# Bad fit
### Conidia discharge
cold <- c(1575, 2019, 1921, 2019, 2323)
medium <- c(2003, NULL, 1510, 1991, 1720)
warm <- c(1742, 1764, NULL, 1470, 1769)
data.frame(cold, medium, warm, colnames("Cold","Medium","Warm"))
data.frame(cold, medium, warm)
?kruskal.test
x <- c(cold, medium, warm)
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
kruskal.test(x,g)
g
boxplot(x)
boxplot(cold,medium,warm)
boxplot(cold,medium,warm, xlabel = c("Cold",
"Medium",
"Warm"))
boxplot(cold,medium,warm, xlabel = c("Cold",
"Medium",
"Warm"))
boxplot(cold,medium,warm, xlabel = c("Cold", "Medium", "Warm"))
?boxplot
boxplot(cold,medium,warm, xlab = c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, names = c("Cold", "Medium", "Warm"))
boxplot(cold,medium,warm, names = c("Cold", "Medium", "Warm"), ylab = "Number of conidia")
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
kruskal.test(x,g)
?anova
anova(x)
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
# Data
cold <- c(1575, 2019, 1921, 2019, 2323)
medium <- c(2003, NULL, 1510, 1991, 1720)
warm <- c(1742, 1764, NULL, 1470, 1769)
groups <- c("cold", "medium", "warm")
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
groups <- c("Cold", "Medium", "Warm")
x <- c(cold, medium, warm)
boxplot(cold,medium,warm, names = groups, ylab = "Number of conidia")
g <- factor(rep(1:3, c(5, 4, 4)),
labels = c("Cold",
"Medium",
"Warm"))
g <- factor(rep(1:3, c(5, 4, 4)),
labels = groups)
#
kruskal.test(x,g)
# Combine the data into a single data frame
data <- data.frame(value = c(cold, medium, warm), group = rep(groups, c(length(cold), length(medium), length(warm))))
# Perform the ANOVA test
anova_results <- anova(lm(value ~ group, data = data))
# Print the summary of the ANOVA test
print(anova_results)
anova(x)
# Print the summary of the ANOVA test
print(anova_results)
p_hat1 <- 0.183
p_hat2 <- 0.225
?prop.test
prop.test(c(xA,xB), c(n,n))
prop.test(0.183,0.225)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n))
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n))
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
?pwr
??pwr
xA <- 22
xB <- 27
n <- 120
prop.test(c(xA,xB), c(n,n), conf.level = 0.95)
prop.test(c(xA,xB), c(n,n), conf.level = 0.95, alternative = "less")
# Power analysis of two proportions with equal n
alpha <- 0.05
power.result <- power.prop.test(n = n, p1 = xA/n, p2 = xB/n, sig.level = alpha, power = NULL)
power.result
source("~/Documents/Github_Desktop/HorseProject/McNemar.R", echo=TRUE)
for_export2 <- signif(t(data.frame(d_section2)),1)
for_export2
for_export2 <- signif(t(data.frame(d_section2)),3)
for_export2
source("~/Documents/Github_Desktop/HorseProject/McNemar.R", echo=TRUE)
